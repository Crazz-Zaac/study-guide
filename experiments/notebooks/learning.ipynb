{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e62816",
   "metadata": {},
   "source": [
    "# LLM with RAG learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff539d",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3424ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-15 12:53:43.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mLoaded 22 texts\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from loguru import logger\n",
    "import pdfplumber\n",
    "\n",
    "dataset = []\n",
    "file_path = 'pa_04_model_selection_via_gap_statistics_and_sampling.pdf'\n",
    "with pdfplumber.open(file_path) as pdf:\n",
    "    for i, page in enumerate(pdf.pages):\n",
    "        dataset.append(page.extract_text())\n",
    "logger.info(f\"Loaded {len(dataset)} texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d770f0",
   "metadata": {},
   "source": [
    "### Implementing Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d2141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eca1b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\n",
    "LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af52c1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-15 13:00:43.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.045454545454545456 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:44.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.09090909090909091 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:44.125\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.13636363636363635 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:44.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.18181818181818182 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:44.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.22727272727272727 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:44.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.2727272727272727 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:44.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.3181818181818182 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:44.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.36363636363636365 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:44.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.4090909090909091 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:44.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.45454545454545453 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:44.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.5 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:44.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.5454545454545454 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:44.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.5909090909090909 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:45.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.6363636363636364 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:45.125\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.6818181818181818 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:45.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.7272727272727273 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:45.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.7727272727272727 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:45.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.8181818181818182 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:45.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.8636363636363636 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:45.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.9090909090909091 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:45.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 0.9545454545454546 to the database\u001b[0m\n",
      "\u001b[32m2025-08-15 13:00:45.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mAdded chunk 1.0 to the database\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "VECTOR_DB: tuple = []  # to store tuple of (chunk, embedding)\n",
    "\n",
    "def add_chunk_to_database(chunk):\n",
    "    embedding:List[float] = ollama.embed(model=EMBEDDING_MODEL, input=chunk)['embeddings'][0]\n",
    "    VECTOR_DB.append((chunk, embedding))\n",
    "\n",
    "for i, chunk in enumerate(dataset):\n",
    "    add_chunk_to_database(chunk=chunk)\n",
    "    logger.info(f\"Added chunk {(i+1)/ len(dataset)} to the database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f92cc26",
   "metadata": {},
   "source": [
    "### Implementing the retrieval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22812f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to calculate the cosine similarity\n",
    "\n",
    "def consine_similarity(a, b):\n",
    "    dot_product = sum([x * y for x, y in zip(a,b)])\n",
    "    norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
    "    norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b751857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_n=3):\n",
    "    query_embedding = ollama.embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "    similarities: List = []\n",
    "    for chunk, embedding in VECTOR_DB:\n",
    "        # calculating the distance \n",
    "        similarity = consine_similarity(query_embedding, embedding)\n",
    "        similarities.append((chunk, similarity))\n",
    "\n",
    "    # sort by similarity in descending order, since higher similarity \n",
    "    # means more relevant chunks\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # return the top N relevant chunks\n",
    "    return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc58f80",
   "metadata": {},
   "source": [
    "### Response Generation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "798583b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m input_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAske me a question: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m retrieved_knowledge \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrieved knowledge: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk, similarity \u001b[38;5;129;01min\u001b[39;00m retrieved_knowledge:\n",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m, in \u001b[0;36mretrieve\u001b[1;34m(query, top_n)\u001b[0m\n\u001b[0;32m      7\u001b[0m     similarities\u001b[38;5;241m.\u001b[39mappend(similarity)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# sort by similarity in descending order, since higher similarity \u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# means more relevant chunks\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43msimilarities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# return the top N relevant chunks\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m similarities[:top_n]\n",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m, in \u001b[0;36mretrieve.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      7\u001b[0m     similarities\u001b[38;5;241m.\u001b[39mappend(similarity)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# sort by similarity in descending order, since higher similarity \u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# means more relevant chunks\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m similarities\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# return the top N relevant chunks\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m similarities[:top_n]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "input_query = input(\"Aske me a question: \")\n",
    "retrieved_knowledge = retrieve(input_query)\n",
    "\n",
    "logger.info(\"Retrieved knowledge: \")\n",
    "for chunk, similarity in retrieved_knowledge:\n",
    "    logger.info(f\"- (similarity: {similarity:.2f}) {chunk}\")\n",
    "\n",
    "context_text = \"\\n\".join([f\"- chunk\" for chunk, _ in retrieved_knowledge])\n",
    "\n",
    "\n",
    "instruction_prompt = f\"\"\"\"You are a helpful chatbot. Use only the following pieces of context to answer the question. \n",
    "Do not make up any new information. \n",
    "Context: {context_text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f71a0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".kolumb (3.10.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
